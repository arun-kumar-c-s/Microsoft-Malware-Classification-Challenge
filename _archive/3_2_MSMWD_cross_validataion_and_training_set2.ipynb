{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6604c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing modules..\n"
     ]
    }
   ],
   "source": [
    "print('importing modules..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#Training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from hyperopt import hp, STATUS_OK, Trials, fmin, tpe\n",
    "# https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3255/models-on-all-features-randomforest-and-xgboost/7/module-6-machine-learning-real-world-case-studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3e3f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "print('loading data...')\n",
    "X = np.load('feature_set2.npy')\n",
    "Y = np.load('target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00aa09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:cv:test :: 6520 : 2174 : 2174\n",
      "Shape of training data (6520, 620)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=11)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train,y_train,test_size=0.25, random_state=11)\n",
    "print('Train:cv:test ::',y_train.shape[0],':',y_cv.shape[0],':',y_test.shape[0])\n",
    "print('Shape of training data', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9667833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test differences - Distribution of classes in train, cv and test set\n",
      "   test_imbalance  train_imbalance  cv_imbalance\n",
      "3        0.280129         0.271472      0.258970\n",
      "2        0.233211         0.228834      0.220331\n",
      "1        0.137994         0.141258      0.147194\n",
      "8        0.109476         0.114110      0.113155\n",
      "9        0.093836         0.091411      0.097976\n",
      "6        0.064397         0.069325      0.073137\n",
      "4        0.040478         0.043558      0.047378\n",
      "7        0.037718         0.035890      0.037718\n",
      "5        0.002760         0.004141      0.004140\n"
     ]
    }
   ],
   "source": [
    "test_class_count = pd.DataFrame(y_test.flatten())[0].value_counts()\n",
    "train_class_count = pd.DataFrame(y_train.flatten())[0].value_counts()\n",
    "cv_class_count = pd.DataFrame(y_cv.flatten())[0].value_counts()\n",
    "class_count = pd.concat((test_class_count,train_class_count),axis=1)\n",
    "class_count.columns = ['test_imbalance','train_imbalance']\n",
    "class_count = pd.concat((class_count,cv_class_count),axis=1)\n",
    "class_count.columns = ['test_imbalance','train_imbalance','cv_imbalance']\n",
    "\n",
    "train_test_difference = class_count/class_count.sum()\n",
    "print(\"Train test differences - Distribution of classes in train, cv and test set\")\n",
    "print(train_test_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfd5daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning..\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 290, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023425612423347248, 'status': 'ok', 'train_loss': 0.011407115434723836, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 290, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 396, 'subsample': 0.5}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02087685188421868, 'status': 'ok', 'train_loss': 0.009884001423295003, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 396, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 378, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023025653743884524, 'status': 'ok', 'train_loss': 0.010746776877673454, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 378, 'subsample': 0.4}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 104, 'subsample': 0.6}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02657286743619554, 'status': 'ok', 'train_loss': 0.010455834729584498, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 104, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 260, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.06698892162191686, 'status': 'ok', 'train_loss': 0.0530363844099486, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 260, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 193, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02214372259672183, 'status': 'ok', 'train_loss': 0.01015654648630132, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 193, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 94, 'subsample': 0.6}              \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02149463554007085, 'status': 'ok', 'train_loss': 0.009787639447378508, 'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 94, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 368, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.019427227538402, 'status': 'ok', 'train_loss': 0.009752976862590267, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 368, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 179, 'subsample': 1}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02184814425666957, 'status': 'ok', 'train_loss': 0.009810508154259268, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 179, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 107, 'subsample': 0.6}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018458384003673407, 'status': 'ok', 'train_loss': 0.009534720344896873, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 107, 'subsample': 0.6}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 384, 'subsample': 0.6}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.026475428759958192, 'status': 'ok', 'train_loss': 0.010508178557739398, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 384, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 166, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02375643349743516, 'status': 'ok', 'train_loss': 0.010279731360041275, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 166, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 85, 'subsample': 0.4}              \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021774449179950313, 'status': 'ok', 'train_loss': 0.010048078677850476, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 85, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 351, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02168470071614595, 'status': 'ok', 'train_loss': 0.010665176613495309, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 351, 'subsample': 0.5}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 260, 'subsample': 1}                   \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02923966831275202, 'status': 'ok', 'train_loss': 0.014324814823812407, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 260, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 312, 'subsample': 0.6}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020018561868550822, 'status': 'ok', 'train_loss': 0.009540143792862166, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 312, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 131, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020279786140288903, 'status': 'ok', 'train_loss': 0.011511180332414817, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 131, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 60, 'subsample': 0.5}              \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02024743104937957, 'status': 'ok', 'train_loss': 0.01004122165693674, 'colsample_bynode': 0.5, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 60, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 124, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018907410272434436, 'status': 'ok', 'train_loss': 0.011322813193230174, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 124, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 331, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020682398530314776, 'status': 'ok', 'train_loss': 0.010295347879465185, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 331, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 107, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018726917452867024, 'status': 'ok', 'train_loss': 0.00965618454240924, 'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 107, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 210, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01867758137582251, 'status': 'ok', 'train_loss': 0.00964587911121564, 'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 210, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018706401249036327, 'status': 'ok', 'train_loss': 0.009658105958093924, 'colsample_bynode': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018391192836998062, 'status': 'ok', 'train_loss': 0.00912838209768504, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018391192836998062, 'status': 'ok', 'train_loss': 0.00912838209768504, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018391192836998062, 'status': 'ok', 'train_loss': 0.00912838209768504, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 150, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 157, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018429803904164835, 'status': 'ok', 'train_loss': 0.009139900692144381, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 157, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018371163231389362, 'status': 'ok', 'train_loss': 0.00912676799290517, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 299, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018671509475577473, 'status': 'ok', 'train_loss': 0.009584000510205958, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 299, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 106, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01839900542325946, 'status': 'ok', 'train_loss': 0.009148730075024468, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 106, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 56, 'subsample': 0.5}              \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021068329168600883, 'status': 'ok', 'train_loss': 0.009964345074784513, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 56, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 228, 'subsample': 0.4}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.019932977688736905, 'status': 'ok', 'train_loss': 0.009879139484826695, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 228, 'subsample': 0.4}\n",
      "{'colsample_bynode': 1, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 250, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01966170545436672, 'status': 'ok', 'train_loss': 0.009801018449731757, 'colsample_bynode': 1, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 250, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 231, 'subsample': 1}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02109872168564039, 'status': 'ok', 'train_loss': 0.009715667950986029, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 231, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 273, 'subsample': 0.5}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02186130276764457, 'status': 'ok', 'train_loss': 0.010086449629915107, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 273, 'subsample': 0.5}\n",
      "{'colsample_bynode': 1, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 218, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021369181792280072, 'status': 'ok', 'train_loss': 0.00948657526546517, 'colsample_bynode': 1, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 218, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.4}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01984264718390866, 'status': 'ok', 'train_loss': 0.010318000005631553, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 175, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828051540625518, 'status': 'ok', 'train_loss': 0.009454544003798904, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 175, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 42, 'subsample': 0.7}              \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018278892297853042, 'status': 'ok', 'train_loss': 0.009557546018518022, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 42, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 42, 'subsample': 1}                \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022183394020881413, 'status': 'ok', 'train_loss': 0.00984623395002492, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 42, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 175, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828051540625518, 'status': 'ok', 'train_loss': 0.009454544003798904, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 175, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 398, 'subsample': 0.4}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01910720949306562, 'status': 'ok', 'train_loss': 0.009773361510059163, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 398, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 97, 'subsample': 0.5}              \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021290602900053707, 'status': 'ok', 'train_loss': 0.009680478849334586, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 97, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 247, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023248387238679655, 'status': 'ok', 'train_loss': 0.010688454880334503, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 247, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 334, 'subsample': 0.6}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.019935754314252943, 'status': 'ok', 'train_loss': 0.009524425427149546, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 334, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 112, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01829093147152248, 'status': 'ok', 'train_loss': 0.009465980169112702, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 112, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 359, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.017919715350828213, 'status': 'ok', 'train_loss': 0.010523712028967356, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 359, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 45, 'subsample': 0.6}                \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.16551458300029137, 'status': 'ok', 'train_loss': 0.1558045257692576, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 45, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 115, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.019725126725513572, 'status': 'ok', 'train_loss': 0.011027072182896423, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 115, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 165, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020131677723017445, 'status': 'ok', 'train_loss': 0.010827717713793853, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 165, 'subsample': 0.5}\n",
      "100%|██████████████████████████████████████████| 50/50 [23:45<00:00, 28.50s/trial, best loss: 0.017919715350828213]\n",
      "        loss status  train_loss  colsample_bynode  learning_rate  max_depth  \\\n",
      "46  0.017920     ok    0.010524               0.6            1.0          3   \n",
      "38  0.018279     ok    0.009558               0.6            0.5          3   \n",
      "40  0.018281     ok    0.009455               0.6            0.5          3   \n",
      "37  0.018281     ok    0.009455               0.6            0.5          3   \n",
      "45  0.018291     ok    0.009466               0.6            0.5          3   \n",
      "27  0.018371     ok    0.009127               0.7            0.5          7   \n",
      "24  0.018391     ok    0.009128               0.7            0.5          7   \n",
      "23  0.018391     ok    0.009128               0.7            0.5          7   \n",
      "25  0.018391     ok    0.009128               0.7            0.5          7   \n",
      "29  0.018399     ok    0.009149               0.7            0.5          7   \n",
      "26  0.018430     ok    0.009140               0.7            0.5          7   \n",
      "9   0.018458     ok    0.009535               0.7            0.5          7   \n",
      "28  0.018672     ok    0.009584               0.7            0.5          3   \n",
      "21  0.018678     ok    0.009646               0.4            0.5          7   \n",
      "22  0.018706     ok    0.009658               0.4            0.5          7   \n",
      "20  0.018727     ok    0.009656               0.4            0.5          7   \n",
      "18  0.018907     ok    0.011323               0.4            1.0          7   \n",
      "41  0.019107     ok    0.009773               0.6            0.5          3   \n",
      "7   0.019427     ok    0.009753               0.5            1.0          7   \n",
      "32  0.019662     ok    0.009801               1.0            0.5          3   \n",
      "48  0.019725     ok    0.011027               0.5            1.0          3   \n",
      "36  0.019843     ok    0.010318               0.7            0.5          5   \n",
      "31  0.019933     ok    0.009879               0.7            0.5          7   \n",
      "44  0.019936     ok    0.009524               0.6            0.5          3   \n",
      "15  0.020019     ok    0.009540               0.6            0.5          3   \n",
      "49  0.020132     ok    0.010828               0.6            1.0          3   \n",
      "17  0.020247     ok    0.010041               0.5            0.5          5   \n",
      "16  0.020280     ok    0.011511               0.7            1.0          5   \n",
      "19  0.020682     ok    0.010295               0.5            1.0          7   \n",
      "1   0.020877     ok    0.009884               0.7            0.5          3   \n",
      "30  0.021068     ok    0.009964               0.7            0.5          3   \n",
      "33  0.021099     ok    0.009716               0.7            0.5          7   \n",
      "42  0.021291     ok    0.009680               0.6            0.5          3   \n",
      "35  0.021369     ok    0.009487               1.0            0.5          1   \n",
      "6   0.021495     ok    0.009788               0.4            0.5          1   \n",
      "13  0.021685     ok    0.010665               0.6            1.0          7   \n",
      "12  0.021774     ok    0.010048               0.6            0.5          1   \n",
      "8   0.021848     ok    0.009811               0.7            0.5          5   \n",
      "34  0.021861     ok    0.010086               0.7            0.5          7   \n",
      "5   0.022144     ok    0.010157               0.4            1.0          1   \n",
      "39  0.022183     ok    0.009846               0.6            0.5          3   \n",
      "2   0.023026     ok    0.010747               0.7            1.0          3   \n",
      "43  0.023248     ok    0.010688               0.6            1.0          3   \n",
      "0   0.023426     ok    0.011407               0.7            1.0          5   \n",
      "11  0.023756     ok    0.010280               0.5            1.0          7   \n",
      "10  0.026475     ok    0.010508               1.0            1.0          5   \n",
      "3   0.026573     ok    0.010456               1.0            1.0          5   \n",
      "14  0.029240     ok    0.014325               1.0            1.0          1   \n",
      "4   0.066989     ok    0.053036               0.6            1.0          1   \n",
      "47  0.165515     ok    0.155805               0.6            1.0          1   \n",
      "\n",
      "    n_estimators  subsample  \n",
      "46           359        0.7  \n",
      "38            42        0.7  \n",
      "40           175        0.7  \n",
      "37           175        0.7  \n",
      "45           112        0.7  \n",
      "27           200        0.7  \n",
      "24           150        0.7  \n",
      "23           150        0.7  \n",
      "25           150        0.7  \n",
      "29           106        0.7  \n",
      "26           157        0.7  \n",
      "9            107        0.6  \n",
      "28           299        0.7  \n",
      "21           210        0.7  \n",
      "22           100        0.7  \n",
      "20           107        0.7  \n",
      "18           124        0.7  \n",
      "41           398        0.4  \n",
      "7            368        0.7  \n",
      "32           250        0.7  \n",
      "48           115        0.4  \n",
      "36           200        0.4  \n",
      "31           228        0.4  \n",
      "44           334        0.6  \n",
      "15           312        0.6  \n",
      "49           165        0.5  \n",
      "17            60        0.5  \n",
      "16           131        0.4  \n",
      "19           331        0.6  \n",
      "1            396        0.5  \n",
      "30            56        0.5  \n",
      "33           231        1.0  \n",
      "42            97        0.5  \n",
      "35           218        0.7  \n",
      "6             94        0.6  \n",
      "13           351        0.5  \n",
      "12            85        0.4  \n",
      "8            179        1.0  \n",
      "34           273        0.5  \n",
      "5            193        0.6  \n",
      "39            42        1.0  \n",
      "2            378        0.4  \n",
      "43           247        1.0  \n",
      "0            290        0.5  \n",
      "11           166        1.0  \n",
      "10           384        0.6  \n",
      "3            104        0.6  \n",
      "14           260        1.0  \n",
      "4            260        0.7  \n",
      "47            45        0.6  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning: https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/\n",
    "# Hyperopt: http://hyperopt.github.io/hyperopt/\n",
    "# Hyperopt gihub: https://github.com/hyperopt/hyperopt/wiki/fmin\n",
    "print('Tuning..')\n",
    "\n",
    "# narrow search on multiple parameters\n",
    "space = { 'max_depth': hp.choice('max_depth',[1,3,5,7]),\n",
    "           'learning_rate': hp.choice('learning_rate',[.5,1]),\n",
    "           'subsample': hp.choice('subsample',[0.4,.5,.6,.7,1]),\n",
    "         'colsample_bynode':hp.choice('colsample_bynode',[0.4,.5,.6,.7,1]),\n",
    "           'n_estimators': hp.choice('n_estimators',np.arange(30,400))}\n",
    "\n",
    "def hyperparameter_tuning(params):\n",
    "    '''return dict should have train loss with key loss for the fmin to minimize. Other params are for analysis'''\n",
    "    clf = XGBClassifier(eval_metric='mlogloss',**params)\n",
    "    print(params, end = \"=>\")\n",
    "    #tick = time.time(),\n",
    "    clf.fit(x_train,y_train.ravel())\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(x_train, y_train.ravel())\n",
    "    train_loss = log_loss(y_train,sig_clf.predict_proba(x_train))\n",
    "    loss = log_loss(y_cv,sig_clf.predict_proba(x_cv))\n",
    "    #print('Time:',time.time() - tick)\n",
    "    dic = {\"loss\": loss, \"status\": STATUS_OK,'train_loss':train_loss}\n",
    "    dic.update(params)\n",
    "    print(dic)\n",
    "    return dic\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning, space = space, trials=trials,  algo=tpe.suggest, max_evals=50)\n",
    "#TRY THIS: use_label_encoder=False when constructing XGBClassifier\n",
    "\n",
    "tuned = pd.DataFrame(trials.results).sort_values(by='loss',axis=0)\n",
    "tuned.to_csv(\"feature_set2_50_trials.csv\")\n",
    "print(tuned)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549aa0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loss</th>\n",
       "      <th>status</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>175</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>175</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>0.018291</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009466</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>106</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26</td>\n",
       "      <td>0.018430</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>157</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28</td>\n",
       "      <td>0.018672</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>210</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22</td>\n",
       "      <td>0.018706</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011323</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>124</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>41</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>398</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>368</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>36</td>\n",
       "      <td>0.019843</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010318</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31</td>\n",
       "      <td>0.019933</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>228</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>44</td>\n",
       "      <td>0.019936</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>334</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>0.020019</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>312</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>49</td>\n",
       "      <td>0.020132</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16</td>\n",
       "      <td>0.020280</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011511</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19</td>\n",
       "      <td>0.020682</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>331</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.020877</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009884</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>396</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.021068</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>0.021099</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>231</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>42</td>\n",
       "      <td>0.021291</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009680</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>12</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>34</td>\n",
       "      <td>0.021861</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>273</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>0.022144</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>0.023026</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010747</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>378</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0.023248</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010688</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>247</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0.023426</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>11</td>\n",
       "      <td>0.023756</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>166</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10</td>\n",
       "      <td>0.026475</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>384</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>0.026573</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>104</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14</td>\n",
       "      <td>0.029240</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.014325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>0.066989</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.053036</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>47</td>\n",
       "      <td>0.165515</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.155805</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0      loss status  train_loss  colsample_bynode  learning_rate  \\\n",
       "0           46  0.017920     ok    0.010524               0.6            1.0   \n",
       "1           38  0.018279     ok    0.009558               0.6            0.5   \n",
       "2           40  0.018281     ok    0.009455               0.6            0.5   \n",
       "3           37  0.018281     ok    0.009455               0.6            0.5   \n",
       "4           45  0.018291     ok    0.009466               0.6            0.5   \n",
       "5           27  0.018371     ok    0.009127               0.7            0.5   \n",
       "6           24  0.018391     ok    0.009128               0.7            0.5   \n",
       "7           23  0.018391     ok    0.009128               0.7            0.5   \n",
       "8           25  0.018391     ok    0.009128               0.7            0.5   \n",
       "9           29  0.018399     ok    0.009149               0.7            0.5   \n",
       "10          26  0.018430     ok    0.009140               0.7            0.5   \n",
       "11           9  0.018458     ok    0.009535               0.7            0.5   \n",
       "12          28  0.018672     ok    0.009584               0.7            0.5   \n",
       "13          21  0.018678     ok    0.009646               0.4            0.5   \n",
       "14          22  0.018706     ok    0.009658               0.4            0.5   \n",
       "15          20  0.018727     ok    0.009656               0.4            0.5   \n",
       "16          18  0.018907     ok    0.011323               0.4            1.0   \n",
       "17          41  0.019107     ok    0.009773               0.6            0.5   \n",
       "18           7  0.019427     ok    0.009753               0.5            1.0   \n",
       "19          32  0.019662     ok    0.009801               1.0            0.5   \n",
       "20          48  0.019725     ok    0.011027               0.5            1.0   \n",
       "21          36  0.019843     ok    0.010318               0.7            0.5   \n",
       "22          31  0.019933     ok    0.009879               0.7            0.5   \n",
       "23          44  0.019936     ok    0.009524               0.6            0.5   \n",
       "24          15  0.020019     ok    0.009540               0.6            0.5   \n",
       "25          49  0.020132     ok    0.010828               0.6            1.0   \n",
       "26          17  0.020247     ok    0.010041               0.5            0.5   \n",
       "27          16  0.020280     ok    0.011511               0.7            1.0   \n",
       "28          19  0.020682     ok    0.010295               0.5            1.0   \n",
       "29           1  0.020877     ok    0.009884               0.7            0.5   \n",
       "30          30  0.021068     ok    0.009964               0.7            0.5   \n",
       "31          33  0.021099     ok    0.009716               0.7            0.5   \n",
       "32          42  0.021291     ok    0.009680               0.6            0.5   \n",
       "33          35  0.021369     ok    0.009487               1.0            0.5   \n",
       "34           6  0.021495     ok    0.009788               0.4            0.5   \n",
       "35          13  0.021685     ok    0.010665               0.6            1.0   \n",
       "36          12  0.021774     ok    0.010048               0.6            0.5   \n",
       "37           8  0.021848     ok    0.009811               0.7            0.5   \n",
       "38          34  0.021861     ok    0.010086               0.7            0.5   \n",
       "39           5  0.022144     ok    0.010157               0.4            1.0   \n",
       "40          39  0.022183     ok    0.009846               0.6            0.5   \n",
       "41           2  0.023026     ok    0.010747               0.7            1.0   \n",
       "42          43  0.023248     ok    0.010688               0.6            1.0   \n",
       "43           0  0.023426     ok    0.011407               0.7            1.0   \n",
       "44          11  0.023756     ok    0.010280               0.5            1.0   \n",
       "45          10  0.026475     ok    0.010508               1.0            1.0   \n",
       "46           3  0.026573     ok    0.010456               1.0            1.0   \n",
       "47          14  0.029240     ok    0.014325               1.0            1.0   \n",
       "48           4  0.066989     ok    0.053036               0.6            1.0   \n",
       "49          47  0.165515     ok    0.155805               0.6            1.0   \n",
       "\n",
       "    max_depth  n_estimators  subsample  \n",
       "0           3           359        0.7  \n",
       "1           3            42        0.7  \n",
       "2           3           175        0.7  \n",
       "3           3           175        0.7  \n",
       "4           3           112        0.7  \n",
       "5           7           200        0.7  \n",
       "6           7           150        0.7  \n",
       "7           7           150        0.7  \n",
       "8           7           150        0.7  \n",
       "9           7           106        0.7  \n",
       "10          7           157        0.7  \n",
       "11          7           107        0.6  \n",
       "12          3           299        0.7  \n",
       "13          7           210        0.7  \n",
       "14          7           100        0.7  \n",
       "15          7           107        0.7  \n",
       "16          7           124        0.7  \n",
       "17          3           398        0.4  \n",
       "18          7           368        0.7  \n",
       "19          3           250        0.7  \n",
       "20          3           115        0.4  \n",
       "21          5           200        0.4  \n",
       "22          7           228        0.4  \n",
       "23          3           334        0.6  \n",
       "24          3           312        0.6  \n",
       "25          3           165        0.5  \n",
       "26          5            60        0.5  \n",
       "27          5           131        0.4  \n",
       "28          7           331        0.6  \n",
       "29          3           396        0.5  \n",
       "30          3            56        0.5  \n",
       "31          7           231        1.0  \n",
       "32          3            97        0.5  \n",
       "33          1           218        0.7  \n",
       "34          1            94        0.6  \n",
       "35          7           351        0.5  \n",
       "36          1            85        0.4  \n",
       "37          5           179        1.0  \n",
       "38          7           273        0.5  \n",
       "39          1           193        0.6  \n",
       "40          3            42        1.0  \n",
       "41          3           378        0.4  \n",
       "42          3           247        1.0  \n",
       "43          5           290        0.5  \n",
       "44          7           166        1.0  \n",
       "45          5           384        0.6  \n",
       "46          5           104        0.6  \n",
       "47          1           260        1.0  \n",
       "48          1           260        0.7  \n",
       "49          1            45        0.6  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"feature_set2_50_trials.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
