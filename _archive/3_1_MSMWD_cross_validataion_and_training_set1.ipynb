{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd573c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing modules..\n"
     ]
    }
   ],
   "source": [
    "print('importing modules..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#Training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from hyperopt import hp, STATUS_OK, Trials, fmin, tpe\n",
    "# https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3255/models-on-all-features-randomforest-and-xgboost/7/module-6-machine-learning-real-world-case-studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e30129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "print('loading data...')\n",
    "X = np.load('feature_set1.npy')\n",
    "Y = np.load('target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e294931a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:cv:test :: 6520 : 2174 : 2174\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=11)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train,y_train,test_size=0.25, random_state=11)\n",
    "print('Train:cv:test ::',y_train.shape[0],':',y_cv.shape[0],':',y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab34371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test differences - Distribution of classes in train, cv and test set\n",
      "   test_imbalance  train_imbalance  cv_imbalance\n",
      "3        0.280129         0.271472      0.258970\n",
      "2        0.233211         0.228834      0.220331\n",
      "1        0.137994         0.141258      0.147194\n",
      "8        0.109476         0.114110      0.113155\n",
      "9        0.093836         0.091411      0.097976\n",
      "6        0.064397         0.069325      0.073137\n",
      "4        0.040478         0.043558      0.047378\n",
      "7        0.037718         0.035890      0.037718\n",
      "5        0.002760         0.004141      0.004140\n"
     ]
    }
   ],
   "source": [
    "test_class_count = pd.DataFrame(y_test.flatten())[0].value_counts()\n",
    "train_class_count = pd.DataFrame(y_train.flatten())[0].value_counts()\n",
    "cv_class_count = pd.DataFrame(y_cv.flatten())[0].value_counts()\n",
    "class_count = pd.concat((test_class_count,train_class_count),axis=1)\n",
    "class_count.columns = ['test_imbalance','train_imbalance']\n",
    "class_count = pd.concat((class_count,cv_class_count),axis=1)\n",
    "class_count.columns = ['test_imbalance','train_imbalance','cv_imbalance']\n",
    "\n",
    "train_test_difference = class_count/class_count.sum()\n",
    "print(\"Train test differences - Distribution of classes in train, cv and test set\")\n",
    "print(train_test_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bfa453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning..\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 143, 'subsample': 0.5}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02253795597828581, 'status': 'ok', 'train_loss': 0.011957510006990631, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 143, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 327, 'subsample': 0.4}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02087359055641676, 'status': 'ok', 'train_loss': 0.010881264450963456, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 327, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 117, 'subsample': 0.6}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020654722208206876, 'status': 'ok', 'train_loss': 0.010370779039529373, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 117, 'subsample': 0.6}\n",
      "{'colsample_bynode': 1, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 385, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02073835160303678, 'status': 'ok', 'train_loss': 0.010634715209482632, 'colsample_bynode': 1, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 385, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 111, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021544133234389584, 'status': 'ok', 'train_loss': 0.012787377289525196, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 111, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 217, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02363812173941839, 'status': 'ok', 'train_loss': 0.012314685022623824, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 217, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 141, 'subsample': 1}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020710199838869695, 'status': 'ok', 'train_loss': 0.010424487244220848, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 141, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 287, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.33836525197780964, 'status': 'ok', 'train_loss': 0.31526961256551833, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 287, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 146, 'subsample': 0.6}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020285067837089277, 'status': 'ok', 'train_loss': 0.010686241180864474, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 223, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 374, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023287849073431777, 'status': 'ok', 'train_loss': 0.010382746117335945, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 374, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 364, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828638982036347, 'status': 'ok', 'train_loss': 0.010962891430826996, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 364, 'subsample': 1}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 168, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023661322026297303, 'status': 'ok', 'train_loss': 0.012308895962762302, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 168, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 263, 'subsample': 0.5}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022476168605874806, 'status': 'ok', 'train_loss': 0.010769226369704798, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 263, 'subsample': 0.5}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 77, 'subsample': 0.4}                  \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023573635575668182, 'status': 'ok', 'train_loss': 0.011759942331898011, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 77, 'subsample': 0.4}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 225, 'subsample': 0.5}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022626404814395432, 'status': 'ok', 'train_loss': 0.012000583652407512, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 225, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 293, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023719186794805218, 'status': 'ok', 'train_loss': 0.012619831783114977, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 293, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 213, 'subsample': 1}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022238819314013718, 'status': 'ok', 'train_loss': 0.010030772969645644, 'colsample_bynode': 0.5, 'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 213, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 373, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.04621633431558832, 'status': 'ok', 'train_loss': 0.03541373692402954, 'colsample_bynode': 0.6, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 373, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 195, 'subsample': 0.4}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020613337482772436, 'status': 'ok', 'train_loss': 0.011339821573903489, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 195, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 296, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02256677929132009, 'status': 'ok', 'train_loss': 0.01131595731386107, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 296, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 146, 'subsample': 1}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02101295974083065, 'status': 'ok', 'train_loss': 0.010363411404799726, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 146, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 177, 'subsample': 1}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021012794578706275, 'status': 'ok', 'train_loss': 0.010363118232219191, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 177, 'subsample': 1}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 255, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022565738756071454, 'status': 'ok', 'train_loss': 0.011314919434454139, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 255, 'subsample': 1}\n",
      "{'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 364, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020315600164210305, 'status': 'ok', 'train_loss': 0.01105357360706564, 'colsample_bynode': 0.6, 'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 364, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 395, 'subsample': 0.6}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023818972680273342, 'status': 'ok', 'train_loss': 0.01153646372770917, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 7, 'n_estimators': 395, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 388, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828682945804985, 'status': 'ok', 'train_loss': 0.01096335763337251, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 388, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 332, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018286772930264004, 'status': 'ok', 'train_loss': 0.010962979138450017, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 332, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 368, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828653118209724, 'status': 'ok', 'train_loss': 0.010963081289536724, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 368, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 318, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828650319458743, 'status': 'ok', 'train_loss': 0.010963042581933834, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 318, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 127, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018285903325850058, 'status': 'ok', 'train_loss': 0.010962410038533104, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 127, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 302, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018287054364135923, 'status': 'ok', 'train_loss': 0.010963479364407213, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 302, 'subsample': 1}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 304, 'subsample': 1}                   \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.023808949448387354, 'status': 'ok', 'train_loss': 0.011252303864380083, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 304, 'subsample': 1}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 291, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.019747565641880512, 'status': 'ok', 'train_loss': 0.01166820681424185, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 291, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 381, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.024411565698279836, 'status': 'ok', 'train_loss': 0.012963083388168584, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 381, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 196, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020668024919943354, 'status': 'ok', 'train_loss': 0.01082404091160321, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 196, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 242, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.021345811429222906, 'status': 'ok', 'train_loss': 0.011225829556888027, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 242, 'subsample': 0.5}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 350, 'subsample': 1}                   \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.22980558037052057, 'status': 'ok', 'train_loss': 0.21895007688436174, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 350, 'subsample': 1}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 104, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02040330677317309, 'status': 'ok', 'train_loss': 0.011767552070065603, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 104, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 392, 'subsample': 0.7}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022921753167528605, 'status': 'ok', 'train_loss': 0.012372889716534963, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 392, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 367, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.018287141877980893, 'status': 'ok', 'train_loss': 0.01096362293842899, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 367, 'subsample': 1}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 316, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.032284881643054666, 'status': 'ok', 'train_loss': 0.02036321885663207, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 316, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 183, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828658542776287, 'status': 'ok', 'train_loss': 0.010962913835020176, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 183, 'subsample': 1}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 365, 'subsample': 0.7}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.022402625566500002, 'status': 'ok', 'train_loss': 0.01173781890235131, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 365, 'subsample': 0.7}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 326, 'subsample': 0.4}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02473264450442829, 'status': 'ok', 'train_loss': 0.012998809304376583, 'colsample_bynode': 0.5, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 326, 'subsample': 0.4}\n",
      "{'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 280, 'subsample': 0.5}               \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01876390444252746, 'status': 'ok', 'train_loss': 0.010998090442452575, 'colsample_bynode': 0.4, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 280, 'subsample': 0.5}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 253, 'subsample': 0.6}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02062946020435429, 'status': 'ok', 'train_loss': 0.010350400496343226, 'colsample_bynode': 0.7, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 253, 'subsample': 0.6}\n",
      "{'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 288, 'subsample': 1}                 \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.01828602442384233, 'status': 'ok', 'train_loss': 0.010962506222193084, 'colsample_bynode': 0.7, 'learning_rate': 1, 'max_depth': 5, 'n_estimators': 288, 'subsample': 1}\n",
      "{'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 39, 'subsample': 1}                    \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.02556493171580036, 'status': 'ok', 'train_loss': 0.011074411649318269, 'colsample_bynode': 1, 'learning_rate': 1, 'max_depth': 3, 'n_estimators': 39, 'subsample': 1}\n",
      "{'colsample_bynode': 0.5, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 383, 'subsample': 0.7}             \n",
      "=>                                                                                                                 \n",
      "{'loss': 0.020786294520645187, 'status': 'ok', 'train_loss': 0.01064349351336331, 'colsample_bynode': 0.5, 'learning_rate': 0.5, 'max_depth': 5, 'n_estimators': 383, 'subsample': 0.7}\n",
      "100%|███████████████████████████████████████| 50/50 [2:36:50<00:00, 188.20s/trial, best loss: 0.018285903325850058]\n",
      "        loss status  train_loss  colsample_bynode  learning_rate  max_depth  \\\n",
      "30  0.018286     ok    0.010962               0.7            1.0          5   \n",
      "47  0.018286     ok    0.010963               0.7            1.0          5   \n",
      "11  0.018286     ok    0.010963               0.7            1.0          5   \n",
      "29  0.018287     ok    0.010963               0.7            1.0          5   \n",
      "28  0.018287     ok    0.010963               0.7            1.0          5   \n",
      "42  0.018287     ok    0.010963               0.7            1.0          5   \n",
      "27  0.018287     ok    0.010963               0.7            1.0          5   \n",
      "26  0.018287     ok    0.010963               0.7            1.0          5   \n",
      "31  0.018287     ok    0.010963               0.7            1.0          5   \n",
      "40  0.018287     ok    0.010964               0.7            1.0          5   \n",
      "45  0.018764     ok    0.010998               0.4            1.0          1   \n",
      "33  0.019748     ok    0.011668               0.4            1.0          5   \n",
      "8   0.020115     ok    0.010520               0.6            0.5          7   \n",
      "9   0.020285     ok    0.010686               0.6            0.5          5   \n",
      "24  0.020316     ok    0.011054               0.6            0.5          7   \n",
      "38  0.020403     ok    0.011768               0.4            1.0          5   \n",
      "19  0.020613     ok    0.011340               0.6            0.5          5   \n",
      "46  0.020629     ok    0.010350               0.7            0.5          5   \n",
      "2   0.020655     ok    0.010371               0.7            0.5          5   \n",
      "35  0.020668     ok    0.010824               0.7            1.0          3   \n",
      "6   0.020710     ok    0.010424               0.7            0.5          3   \n",
      "3   0.020738     ok    0.010635               1.0            0.5          3   \n",
      "49  0.020786     ok    0.010643               0.5            0.5          5   \n",
      "1   0.020874     ok    0.010881               0.6            0.5          3   \n",
      "22  0.021013     ok    0.010363               0.7            0.5          7   \n",
      "21  0.021013     ok    0.010363               0.7            0.5          7   \n",
      "36  0.021346     ok    0.011226               0.7            1.0          5   \n",
      "4   0.021544     ok    0.012787               0.4            1.0          7   \n",
      "17  0.022239     ok    0.010031               0.5            0.5          3   \n",
      "43  0.022403     ok    0.011738               1.0            1.0          3   \n",
      "13  0.022476     ok    0.010769               0.7            0.5          1   \n",
      "0   0.022538     ok    0.011958               1.0            1.0          5   \n",
      "23  0.022566     ok    0.011315               0.4            1.0          7   \n",
      "20  0.022567     ok    0.011316               0.4            1.0          7   \n",
      "15  0.022626     ok    0.012001               1.0            1.0          5   \n",
      "39  0.022922     ok    0.012373               0.5            1.0          3   \n",
      "10  0.023288     ok    0.010383               0.7            0.5          1   \n",
      "14  0.023574     ok    0.011760               1.0            1.0          3   \n",
      "5   0.023638     ok    0.012315               0.5            1.0          5   \n",
      "12  0.023661     ok    0.012309               0.5            1.0          5   \n",
      "16  0.023719     ok    0.012620               0.6            1.0          7   \n",
      "32  0.023809     ok    0.011252               1.0            1.0          5   \n",
      "25  0.023819     ok    0.011536               0.7            1.0          7   \n",
      "34  0.024412     ok    0.012963               0.5            1.0          5   \n",
      "44  0.024733     ok    0.012999               0.5            1.0          5   \n",
      "48  0.025565     ok    0.011074               1.0            1.0          3   \n",
      "41  0.032285     ok    0.020363               0.7            1.0          1   \n",
      "18  0.046216     ok    0.035414               0.6            1.0          1   \n",
      "37  0.229806     ok    0.218950               1.0            1.0          1   \n",
      "7   0.338365     ok    0.315270               0.6            1.0          1   \n",
      "\n",
      "    n_estimators  subsample  \n",
      "30           127        1.0  \n",
      "47           288        1.0  \n",
      "11           364        1.0  \n",
      "29           318        1.0  \n",
      "28           368        1.0  \n",
      "42           183        1.0  \n",
      "27           332        1.0  \n",
      "26           388        1.0  \n",
      "31           302        1.0  \n",
      "40           367        1.0  \n",
      "45           280        0.5  \n",
      "33           291        0.7  \n",
      "8            146        0.6  \n",
      "9            223        0.5  \n",
      "24           364        0.7  \n",
      "38           104        0.4  \n",
      "19           195        0.4  \n",
      "46           253        0.6  \n",
      "2            117        0.6  \n",
      "35           196        1.0  \n",
      "6            141        1.0  \n",
      "3            385        0.5  \n",
      "49           383        0.7  \n",
      "1            327        0.4  \n",
      "22           177        1.0  \n",
      "21           146        1.0  \n",
      "36           242        0.5  \n",
      "4            111        0.4  \n",
      "17           213        1.0  \n",
      "43           365        0.7  \n",
      "13           263        0.5  \n",
      "0            143        0.5  \n",
      "23           255        1.0  \n",
      "20           296        1.0  \n",
      "15           225        0.5  \n",
      "39           392        0.7  \n",
      "10           374        0.7  \n",
      "14            77        0.4  \n",
      "5            217        0.6  \n",
      "12           168        0.6  \n",
      "16           293        0.6  \n",
      "32           304        1.0  \n",
      "25           395        0.6  \n",
      "34           381        0.4  \n",
      "44           326        0.4  \n",
      "48            39        1.0  \n",
      "41           316        0.5  \n",
      "18           373        0.6  \n",
      "37           350        1.0  \n",
      "7            287        0.7  \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning: https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/\n",
    "# Hyperopt: http://hyperopt.github.io/hyperopt/\n",
    "# Hyperopt gihub: https://github.com/hyperopt/hyperopt/wiki/fmin\n",
    "print('Tuning..')\n",
    "\n",
    "# narrow search on multiple parameters\n",
    "space = { 'max_depth': hp.choice('max_depth',[1,3,5,7]),\n",
    "           'learning_rate': hp.choice('learning_rate',[.5,1]),\n",
    "           'subsample': hp.choice('subsample',[0.4,.5,.6,.7,1]),\n",
    "         'colsample_bynode':hp.choice('colsample_bynode',[0.4,.5,.6,.7,1]),\n",
    "           'n_estimators': hp.choice('n_estimators',np.arange(30,400))}\n",
    "\n",
    "def hyperparameter_tuning(params):\n",
    "    '''return dict should have train loss with key loss for the fmin to minimize. Other params are for analysis'''\n",
    "    clf = XGBClassifier(eval_metric='mlogloss',**params)\n",
    "    print(params, end = \"=>\")\n",
    "    #tick = time.time(),\n",
    "    clf.fit(x_train,y_train.ravel())\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(x_train, y_train.ravel())\n",
    "    train_loss = log_loss(y_train,sig_clf.predict_proba(x_train))\n",
    "    loss = log_loss(y_cv,sig_clf.predict_proba(x_cv))\n",
    "    #print('Time:',time.time() - tick)\n",
    "    dic = {\"loss\": loss, \"status\": STATUS_OK,'train_loss':train_loss}\n",
    "    dic.update(params)\n",
    "    print(dic)\n",
    "    return dic\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning, space = space, trials=trials,  algo=tpe.suggest, max_evals=50)\n",
    "#TRY THIS: use_label_encoder=False when constructing XGBClassifier\n",
    "\n",
    "tuned = pd.DataFrame(trials.results).sort_values(by='loss',axis=0)\n",
    "tuned.to_csv(\"feature_set1_50_trials.csv\")\n",
    "print(tuned)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f9eefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loss</th>\n",
       "      <th>status</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>127</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>288</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>364</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>318</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>368</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>183</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>332</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>388</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>302</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>367</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45</td>\n",
       "      <td>0.018764</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33</td>\n",
       "      <td>0.019748</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>291</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010520</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>146</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>0.020285</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010686</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>364</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38</td>\n",
       "      <td>0.020403</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>104</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>0.020613</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>195</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>46</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>253</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35</td>\n",
       "      <td>0.020668</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010424</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>141</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>385</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>49</td>\n",
       "      <td>0.020786</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>383</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>327</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>146</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>36</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>242</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012787</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>111</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>213</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>43</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>365</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>263</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>23</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20</td>\n",
       "      <td>0.022567</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>296</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>15</td>\n",
       "      <td>0.022626</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>225</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>39</td>\n",
       "      <td>0.022922</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>392</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>0.023288</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>374</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14</td>\n",
       "      <td>0.023574</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>0.023638</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012315</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>217</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>12</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012309</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>168</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>0.023719</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012620</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>293</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>395</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>34</td>\n",
       "      <td>0.024412</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>381</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.024733</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>326</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>48</td>\n",
       "      <td>0.025565</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>41</td>\n",
       "      <td>0.032285</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>316</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>18</td>\n",
       "      <td>0.046216</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>37</td>\n",
       "      <td>0.229806</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.218950</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>350</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7</td>\n",
       "      <td>0.338365</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.315270</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0      loss status  train_loss  colsample_bynode  learning_rate  \\\n",
       "0           30  0.018286     ok    0.010962               0.7            1.0   \n",
       "1           47  0.018286     ok    0.010963               0.7            1.0   \n",
       "2           11  0.018286     ok    0.010963               0.7            1.0   \n",
       "3           29  0.018287     ok    0.010963               0.7            1.0   \n",
       "4           28  0.018287     ok    0.010963               0.7            1.0   \n",
       "5           42  0.018287     ok    0.010963               0.7            1.0   \n",
       "6           27  0.018287     ok    0.010963               0.7            1.0   \n",
       "7           26  0.018287     ok    0.010963               0.7            1.0   \n",
       "8           31  0.018287     ok    0.010963               0.7            1.0   \n",
       "9           40  0.018287     ok    0.010964               0.7            1.0   \n",
       "10          45  0.018764     ok    0.010998               0.4            1.0   \n",
       "11          33  0.019748     ok    0.011668               0.4            1.0   \n",
       "12           8  0.020115     ok    0.010520               0.6            0.5   \n",
       "13           9  0.020285     ok    0.010686               0.6            0.5   \n",
       "14          24  0.020316     ok    0.011054               0.6            0.5   \n",
       "15          38  0.020403     ok    0.011768               0.4            1.0   \n",
       "16          19  0.020613     ok    0.011340               0.6            0.5   \n",
       "17          46  0.020629     ok    0.010350               0.7            0.5   \n",
       "18           2  0.020655     ok    0.010371               0.7            0.5   \n",
       "19          35  0.020668     ok    0.010824               0.7            1.0   \n",
       "20           6  0.020710     ok    0.010424               0.7            0.5   \n",
       "21           3  0.020738     ok    0.010635               1.0            0.5   \n",
       "22          49  0.020786     ok    0.010643               0.5            0.5   \n",
       "23           1  0.020874     ok    0.010881               0.6            0.5   \n",
       "24          22  0.021013     ok    0.010363               0.7            0.5   \n",
       "25          21  0.021013     ok    0.010363               0.7            0.5   \n",
       "26          36  0.021346     ok    0.011226               0.7            1.0   \n",
       "27           4  0.021544     ok    0.012787               0.4            1.0   \n",
       "28          17  0.022239     ok    0.010031               0.5            0.5   \n",
       "29          43  0.022403     ok    0.011738               1.0            1.0   \n",
       "30          13  0.022476     ok    0.010769               0.7            0.5   \n",
       "31           0  0.022538     ok    0.011958               1.0            1.0   \n",
       "32          23  0.022566     ok    0.011315               0.4            1.0   \n",
       "33          20  0.022567     ok    0.011316               0.4            1.0   \n",
       "34          15  0.022626     ok    0.012001               1.0            1.0   \n",
       "35          39  0.022922     ok    0.012373               0.5            1.0   \n",
       "36          10  0.023288     ok    0.010383               0.7            0.5   \n",
       "37          14  0.023574     ok    0.011760               1.0            1.0   \n",
       "38           5  0.023638     ok    0.012315               0.5            1.0   \n",
       "39          12  0.023661     ok    0.012309               0.5            1.0   \n",
       "40          16  0.023719     ok    0.012620               0.6            1.0   \n",
       "41          32  0.023809     ok    0.011252               1.0            1.0   \n",
       "42          25  0.023819     ok    0.011536               0.7            1.0   \n",
       "43          34  0.024412     ok    0.012963               0.5            1.0   \n",
       "44          44  0.024733     ok    0.012999               0.5            1.0   \n",
       "45          48  0.025565     ok    0.011074               1.0            1.0   \n",
       "46          41  0.032285     ok    0.020363               0.7            1.0   \n",
       "47          18  0.046216     ok    0.035414               0.6            1.0   \n",
       "48          37  0.229806     ok    0.218950               1.0            1.0   \n",
       "49           7  0.338365     ok    0.315270               0.6            1.0   \n",
       "\n",
       "    max_depth  n_estimators  subsample  \n",
       "0           5           127        1.0  \n",
       "1           5           288        1.0  \n",
       "2           5           364        1.0  \n",
       "3           5           318        1.0  \n",
       "4           5           368        1.0  \n",
       "5           5           183        1.0  \n",
       "6           5           332        1.0  \n",
       "7           5           388        1.0  \n",
       "8           5           302        1.0  \n",
       "9           5           367        1.0  \n",
       "10          1           280        0.5  \n",
       "11          5           291        0.7  \n",
       "12          7           146        0.6  \n",
       "13          5           223        0.5  \n",
       "14          7           364        0.7  \n",
       "15          5           104        0.4  \n",
       "16          5           195        0.4  \n",
       "17          5           253        0.6  \n",
       "18          5           117        0.6  \n",
       "19          3           196        1.0  \n",
       "20          3           141        1.0  \n",
       "21          3           385        0.5  \n",
       "22          5           383        0.7  \n",
       "23          3           327        0.4  \n",
       "24          7           177        1.0  \n",
       "25          7           146        1.0  \n",
       "26          5           242        0.5  \n",
       "27          7           111        0.4  \n",
       "28          3           213        1.0  \n",
       "29          3           365        0.7  \n",
       "30          1           263        0.5  \n",
       "31          5           143        0.5  \n",
       "32          7           255        1.0  \n",
       "33          7           296        1.0  \n",
       "34          5           225        0.5  \n",
       "35          3           392        0.7  \n",
       "36          1           374        0.7  \n",
       "37          3            77        0.4  \n",
       "38          5           217        0.6  \n",
       "39          5           168        0.6  \n",
       "40          7           293        0.6  \n",
       "41          5           304        1.0  \n",
       "42          7           395        0.6  \n",
       "43          5           381        0.4  \n",
       "44          5           326        0.4  \n",
       "45          3            39        1.0  \n",
       "46          1           316        0.5  \n",
       "47          1           373        0.6  \n",
       "48          1           350        1.0  \n",
       "49          1           287        0.7  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"feature_set1_50_trials.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
